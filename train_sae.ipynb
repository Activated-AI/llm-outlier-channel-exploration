{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#----- imports --------\n",
    "\n",
    "import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import wandb\n",
    "import os\n",
    "import tokenizers\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import tqdm\n",
    "\n",
    "\n",
    "\n",
    "# Set variables as locals\n",
    "learning_rate = 1e-3\n",
    "sae_size = 2**14\n",
    "sae_learning_rate = 5e-5\n",
    "sae_sparsity_penalty = 250\n",
    "model_embedding_layer = 6\n",
    "eval_interval = 500\n",
    "max_iters = 60000\n",
    "H = 32  # hidden dimension size\n",
    "B = 64\n",
    "T = 256\n",
    "C = 256\n",
    "feedforward_factor = 3\n",
    "n_heads = 8\n",
    "n_layers = 12\n",
    "tokenizer_vocab_size = 2**13\n",
    "git_hash = os.popen(\"git rev-parse HEAD\").read().strip()\n",
    "\n",
    "# Create config from local variables\n",
    "config = {\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"sae_size\": sae_size,\n",
    "    \"sae_learning_rate\": sae_learning_rate,\n",
    "    \"sae_sparsity_penalty\": sae_sparsity_penalty,\n",
    "    \"model_embedding_layer\": model_embedding_layer,\n",
    "    \"eval_interval\": eval_interval,\n",
    "    \"max_iters\": max_iters,\n",
    "    \"H\": H,\n",
    "    \"B\": B,\n",
    "    \"T\": T,\n",
    "    \"C\": C,\n",
    "    \"feedforward_factor\": feedforward_factor,\n",
    "    \"n_heads\": n_heads,\n",
    "    \"n_layers\": n_layers,\n",
    "    \"tokenizer_vocab_size\": tokenizer_vocab_size,\n",
    "    \"git_hash\": git_hash\n",
    "}\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "import-exclude"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B disabled.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!wandb disabled\n",
    "# !wandb enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "device= 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.set_default_device(device)\n",
    "assert device == 'cuda', \"This notebook is not optimized for CPU\"\n",
    "\n",
    "# initial\n",
    "for k,v in config.items():\n",
    "    locals ()[k] = v\n",
    "\n",
    "\n",
    "!wandb disabled\n",
    "\n",
    "wandb.init(\n",
    "   project = \"scaling-monosemanticity-vanilla\",\n",
    "   config = config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 0.9200, 0.9200, 1.0000], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "a = torch.tensor([[1.25,2,3,4], # predicted example 1\n",
    "                  [1.25,2.4,3.4,4.5]], dtype=torch.float32) # predicted example 2\n",
    "\n",
    "b = torch.tensor([[1,   2,   3,   4  ], # actual example 1\n",
    "                  [1.5, 2.5, 3.5, 4.5]], dtype=torch.float32) # actual example 2\n",
    "\n",
    "def r2_per_channel(predicted, actual):\n",
    "    channel_means = torch.mean(actual, dim=-2)\n",
    "    avg_squared_error_per_channel = torch.mean((actual - channel_means)**2, dim=-2)\n",
    "    avg_squared_error_predicted = torch.mean((predicted - actual)**2, dim=-2)\n",
    "    return 1 - avg_squared_error_predicted / avg_squared_error_per_channel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "sample_feature_activations = torch.tensor(\n",
    "    [\n",
    "        [0, 0, 0, 0],\n",
    "        [0, 0, 1, 0],\n",
    "        [0, 3, 0, 1],\n",
    "        [1, 1, 0, 1]\n",
    "    ]\n",
    ")\n",
    "\n",
    "def active_features_per_token(feature_activations):\n",
    "    nonzero_counts = (feature_activations != 0).sum(dim=-1).to(torch.float32)\n",
    "    return nonzero_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseAutoEncoder(nn.Module):\n",
    "    def __init__(self, activations_dim, sparse_dim):\n",
    "        super().__init__()\n",
    "        self.activations_dim = activations_dim\n",
    "        encoder_weight = torch.randn(activations_dim, sparse_dim)\n",
    "        decoder_weight = torch.randn(sparse_dim, activations_dim)\n",
    "        self.encoder_bias = nn.Parameter(torch.zeros(sparse_dim))\n",
    "        self.decoder_bias = nn.Parameter(torch.zeros(activations_dim))\n",
    "        self.sparse_dim = sparse_dim\n",
    "        self.sparsity_penalty = sae_sparsity_penalty\n",
    "\n",
    "        # set the encoder_weight to have the activations dim to be normalized to have l2 norm randomly between 0.05 and 1\n",
    "        direction_lengths = torch.rand(sparse_dim) * 0.95 + 0.05\n",
    "        # normalize the encoder_weight along columns (dim -2) to have l2 norm of 1\n",
    "        encoder_weight = F.normalize(encoder_weight, p=2, dim=0)\n",
    "        # multiply the column norms by the direction_lengths\n",
    "        encoder_weight = encoder_weight * direction_lengths\n",
    "        # make the decoder weight be the transpose of the encoder_weight\n",
    "        decoder_weight = torch.transpose(encoder_weight, 0, 1)\n",
    "\n",
    "        self.encoder_weight = nn.Parameter(encoder_weight)\n",
    "        self.decoder_weight = nn.Parameter(decoder_weight)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # preprocessing normalization\n",
    "        # now on average any embedding has euclidian length 1\n",
    "\n",
    "        encoded = F.relu(x @ self.encoder_weight + self.encoder_bias) # all act. are positive\n",
    "        decoded = encoded @ self.decoder_weight + self.decoder_bias\n",
    "\n",
    "        reconstruction_l2_loss = F.mse_loss(x, decoded)\n",
    "\n",
    "        # every row in the tall decoder matrix\n",
    "        # is the \"sum\" of the total influence of a feature on the output\n",
    "        # the l2 norm of that row is the \"influence\" of that feature on that output\n",
    "        # calculate that, store as row\n",
    "        decoder_l2 = torch.linalg.norm(self.decoder_weight, dim=-1)\n",
    "        # the feature activation is the sparse activation * it's influence on output\n",
    "        feature_activations = (encoded) * decoder_l2\n",
    "        # sum of feature activations\n",
    "        # divide by the batch size * sequence length\n",
    "        # should work if there is no batch dimension\n",
    "        if x.ndim == 3:\n",
    "            batch_dim, sequence_dim, _ = x.shape\n",
    "        elif x.ndim == 2:\n",
    "            batch_dim = 1\n",
    "            sequence_dim, _ = x.shape\n",
    "        elif x.ndim == 1:\n",
    "            batch_dim = 1\n",
    "            sequence_dim = 1\n",
    "        else:\n",
    "            raise ValueError(f\"x has {x.ndim} dimensions, but it should have 1, 2, or 3\")\n",
    "        \n",
    "        sparsity_loss = torch.sum(feature_activations) * self.sparsity_penalty / (batch_dim * sequence_dim * self.sparse_dim)\n",
    "\n",
    "        total_loss = reconstruction_l2_loss + sparsity_loss\n",
    "\n",
    "        return {\"encoded\": encoded, \"decoded\": decoded, 'feature_activations': feature_activations, \"reconstruction_loss\": reconstruction_l2_loss, \"sparsity_loss\": sparsity_loss, \"total_loss\": total_loss}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sae = SparseAutoEncoder(C, sae_size)\n",
    "optimizer = torch.optim.Adam(sae.parameters(), lr=sae_learning_rate)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 8405248\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total number of parameters\n",
    "total_params = sum(p.numel() for p in sae.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tensor(filepath):\n",
    "    # load the .pt tensor\n",
    "    tensor = torch.load(filepath)\n",
    "    tensor = torch.cat(tensor, dim=0)\n",
    "    tensor = tensor.to(device)\n",
    "    return tensor\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reconstruction_loss': 2745.4295654296875,\n",
       " 'sparsity_loss': 175.28909301757812,\n",
       " 'total_loss': 2920.71875,\n",
       " 'r2_per_channel': -326.84381103515625,\n",
       " 'active_features_per_token': 8154.78125}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_sae_loss(eval_iters, tensor):\n",
    "    sae_loss = 0\n",
    "    sae_sparsity_loss = 0\n",
    "    sae_reconstruction_loss = 0\n",
    "    count = 0\n",
    "    avg_r2_per_channel_sum = 0\n",
    "    avg_active_features_per_token_sum = 0\n",
    "    \n",
    "    for i in range(0, eval_iters, B):\n",
    "        count += 1\n",
    "        start = i\n",
    "        end = i+B\n",
    "        assert tensor.shape[0] >= end, f\"too many eval_iters\"\n",
    "        sample = tensor[start:end]\n",
    "        sae_output = sae(sample)\n",
    "        sae_loss += sae_output['total_loss'].item()\n",
    "        sae_sparsity_loss += sae_output['sparsity_loss'].item()\n",
    "        sae_reconstruction_loss += sae_output['reconstruction_loss'].item()\n",
    "        avg_r2_per_channel_sum += torch.mean(r2_per_channel(sae_output['decoded'], sample)).item()\n",
    "        avg_active_features_per_token_sum += torch.mean(active_features_per_token(sae_output['feature_activations'])).item()\n",
    "    avg_loss = sae_loss/count\n",
    "    avg_sparsity_loss = sae_sparsity_loss/count\n",
    "    avg_reconstruction_loss = sae_reconstruction_loss/count\n",
    "    avg_r2_per_channel = avg_r2_per_channel_sum/count\n",
    "    avg_active_features_per_token = avg_active_features_per_token_sum/count\n",
    "    return {\"reconstruction_loss\": avg_reconstruction_loss, \"sparsity_loss\": avg_sparsity_loss, \"total_loss\": avg_loss, \"r2_per_channel\": avg_r2_per_channel, \"active_features_per_token\": avg_active_features_per_token}\n",
    "    \n",
    "\n",
    "\n",
    "estimate_sae_loss(100, load_tensor(\"residuals/residuals_train_1.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filepaths = []\n",
    "val_filepaths = []\n",
    "for file in os.listdir(f'residuals'):\n",
    "    if file.startswith(f\"residuals_train\"):\n",
    "        train_filepaths.append(f\"residuals/{file}\")\n",
    "    elif file.startswith(f\"residuals_val\"):\n",
    "        val_filepaths.append(f\"residuals/{file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop():\n",
    "    optimizer = torch.optim.Adam(sae.parameters(), lr=sae_learning_rate)\n",
    "    num_epochs = 1\n",
    "    logging_interval = 50000\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for filepath in train_filepaths:\n",
    "            val_residuals_tensor = load_tensor(random.choice(val_filepaths))\n",
    "            print(f\"val loss on next datafile\")\n",
    "            val_data = estimate_sae_loss(1000, val_residuals_tensor)# keys: reconstruction_loss, sparsity_loss, total_loss\n",
    "            wandb.log({\"val_reconstruction_loss\": val_data['reconstruction_loss'], \"val_sparsity_loss\": val_data['sparsity_loss'], \"val_total_loss\": val_data['total_loss'], \"val_r2_per_channel\": val_data['r2_per_channel'], \"val_active_features_per_token\": val_data['active_features_per_token']})\n",
    "            del val_residuals_tensor\n",
    "            residuals_tensor = load_tensor(filepath)\n",
    "            print(f\"train loss on next datafile\")\n",
    "            train_data = estimate_sae_loss(1000, residuals_tensor)# keys: reconstruction_loss, sparsity_loss, total_loss\n",
    "            # wandb.log({\"train_reconstruction_loss\": train_data['reconstruction_loss'], \"train_sparsity_loss\": train_data['sparsity_loss'], \"train_total_loss\": train_data['total_loss']})\n",
    "            print(f\"training on {filepath}\")\n",
    "\n",
    "            for i in tqdm.tqdm(range(0, residuals_tensor.shape[0]-B, B)):\n",
    "                start = i\n",
    "                end = i+B\n",
    "                assert residuals_tensor.shape[0] >= end, f\"too many train samples\"\n",
    "                sample = residuals_tensor[start:end]\n",
    "                optimizer.zero_grad()\n",
    "                sae_output = sae(sample)\n",
    "                sae_reconstruction_loss = sae_output['reconstruction_loss']\n",
    "                sae_sparsity_loss = sae_output['sparsity_loss']\n",
    "                total_loss = sae_reconstruction_loss + sae_sparsity_loss\n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "                if i % logging_interval == 0:\n",
    "                    pass\n",
    "                    wandb.log({\"frequent_reconstruction_loss\": sae_reconstruction_loss, \"frequent_sparsity_loss\": sae_sparsity_loss, \"frequent_total_loss\": total_loss})\n",
    "    wandb.finish()\n",
    "    torch.save(sae.state_dict(), 'sae_weights.pth')\n",
    "    results = estimate_sae_loss(200, load_tensor(\"residuals/residuals_val_12.pt\"))\n",
    "    return results # keys: reconstruction_loss, sparsity_loss, total_loss, r2_per_channel, active_features_per_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "import-exclude"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss on next datafile\n",
      "train loss on next datafile\n",
      "training on residuals/residuals_train_5.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51187/51187 [01:48<00:00, 469.92it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'reconstruction_loss': 1.5820788443088531,\n",
       " 'sparsity_loss': 3.3748157024383545,\n",
       " 'total_loss': 4.956894636154175,\n",
       " 'r2_per_channel': 0.3508022204041481,\n",
       " 'active_features_per_token': 49.24609375}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loop() # keys: reconstruction_loss, sparsity_loss, total_loss, r2_per_channel, active_features_per_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAE model loaded from sae_weights.pth\n"
     ]
    }
   ],
   "source": [
    "# # Load the SAE model weights\n",
    "# sae_weights_path = 'sae_weights.pth'\n",
    "# sae.load_state_dict(torch.load(sae_weights_path))\n",
    "# sae.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# print(f\"SAE model loaded from {sae_weights_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
