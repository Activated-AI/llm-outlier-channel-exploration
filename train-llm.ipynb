{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load dataset\n",
    "\n",
    "# from datasets import load_dataset\n",
    "# from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "# tokenizer = ByteLevelBPETokenizer()\n",
    "# dataset = load_dataset(\"roneneldan/TinyStories\")\n",
    "\n",
    "# # Specify the split you want to save (e.g., \"train\", \"validation\", \"test\")\n",
    "# split = \"train\"\n",
    "\n",
    "# # Get the desired split from the dataset\n",
    "# subset = dataset[split]\n",
    "\n",
    "# # Save the subset to a text file\n",
    "# subset.to_csv(\"tinystories-train.txt\", sep=\"\\t\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----- imports --------\n",
    "\n",
    "import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "import wandb\n",
    "import os\n",
    "import tokenizers\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "\n",
    "device= 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.set_default_device(device)\n",
    "assert device == 'cuda', \"This notebook is not optimized for CPU\"\n",
    "\n",
    "config = {\n",
    "    \"learning_rate\": 2e-3,\n",
    "    \"sae_learning_rate\": 5e-5,\n",
    "    \"model_embedding_layer\": 6,\n",
    "    \"eval_interval\": 500,\n",
    "    \"max_iters\": 60000, \n",
    "    \"H\": 32, # hidden dimension size\n",
    "    \"B\": 128,\n",
    "    \"T\": 256,\n",
    "    \"C\": 256,\n",
    "    \"feedforward_factor\": 3,\n",
    "    \"n_heads\": 8,\n",
    "    \"n_layers\": 12,\n",
    "    \"tokenizer_vocab_size\": 2**13,\n",
    "    \"git_hash\": os.popen(\"git rev-parse HEAD\").read().strip()\n",
    "}\n",
    "\n",
    "# initial\n",
    "for k,v in config.items():\n",
    "    locals ()[k] = v\n",
    "\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "#wandb.init(\n",
    "#    project = \"tinystories\",\n",
    "#    config = config,\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# stories_data = []\n",
    "# data_dir = './data'\n",
    "# for filename in os.listdir(data_dir):\n",
    "#     file_path = os.path.join(data_dir, filename)\n",
    "#     if filename.endswith('.json'):\n",
    "#         with open(file_path, 'r', encoding='utf-8') as f:\n",
    "#             data = json.load(f)\n",
    "#             stories_data.extend(data)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load the tinystories tokenizer\n",
    "# tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "#     \"./tiny-stories-bpe-vocab.json\", \n",
    "#     \"./tiny-stories-bpe-merges.txt\"\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# def encode(text):\n",
    "#     return torch.tensor(tokenizer.encode(text).ids, dtype=torch.int64)\n",
    "# def decode(encoded_text):\n",
    "#     return tokenizer.decode(encoded_text.tolist())\n",
    "\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# encoded_stories = [encode(story['story']) for story in tqdm(stories_data, desc=\"Encoding stories\")]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the encoded stories to a file\n",
    "# torch.save(encoded_stories, 'encoded-stories.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('tinystories-train.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1922767089\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "479051742.25"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1916206969/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in lines:  14815490\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in lines: \", len(text.split('\\n')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\n",
      "Lily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\n",
      "Together, they shared the needle and sewed the button on Lily's shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.\n",
      "<|endoftext|>\n",
      "Once upon a time, there was a little car named Beep. Beep loved to go fast and play in the sun. Beep was a healthy car because he always had good fuel. Good fuel made Beep happy and strong.\n",
      "One day, Beep was driving in the park when he saw a big tree. The tree had many leaves that were\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths = ['tinystories-train.txt']\n",
    "# tokenizer = tokenizers.ByteLevelBPETokenizer()\n",
    "\n",
    "# tokenizer.train(files=paths, vocab_size=tokenizer_vocab_size, min_frequency=2)\n",
    "\n",
    "# tokenizer.save_model('.', 'tiny-stories-bpe')\n",
    "\n",
    "\n",
    "\n",
    "# enc = tokenizer.encode(\"She sells sea shells by the sea shore!\")\n",
    "# tokenizer.decode(enc.ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "    \"./tiny-stories-bpe-vocab.json\", \n",
    "    \"./tiny-stories-bpe-merges.txt\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6099]\n",
      "hello\n",
      "vocab size:  8192\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def encode(text):\n",
    "    return tokenizer.encode(text).ids\n",
    "def decode(encoded_text):\n",
    "    return tokenizer.decode(encoded_text)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def batch_encode(text, batch_size):\n",
    "    tokens = []\n",
    "    for i in tqdm(range(0, len(text), batch_size)):\n",
    "        tokens.extend(encode(text[i:i+batch_size]))\n",
    "    return tokens\n",
    "\n",
    "\n",
    "hello_encoded = encode(\"hello\")\n",
    "print(hello_encoded)\n",
    "print(decode(hello_encoded))\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "print(\"vocab size: \", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:00<00:00, 49.69it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 58.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory used by sample_encoded:  1.2849769592285156 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sample_text = text[:200000]\n",
    "sample_encoded = batch_encode(sample_text, 20000)\n",
    "\n",
    "# get the amount of memory used by sample_encoded\n",
    "def recursive_memory_usage(python_obj):\n",
    "    if isinstance(python_obj, (str, int, float)):\n",
    "        return python_obj.__sizeof__()\n",
    "    if isinstance(python_obj, dict):\n",
    "        return sum([recursive_memory_usage(v) for v in python_obj.values()])\n",
    "    if isinstance(python_obj, list):\n",
    "        return sum([recursive_memory_usage(v) for v in python_obj])\n",
    "    return python_obj.__sizeof__()\n",
    "\n",
    "print(\"memory used by sample_encoded: \", recursive_memory_usage(sample_encoded) / 1024**2, \"MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  10000\n",
      "length of dataset in tokens:  2440\n",
      "characters per token:  4.098360655737705\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters: \", len(text[:10000]))\n",
    "print(\"length of dataset in tokens: \", len(encode(text[:10000])))\n",
    "chars_per_token = len(text[:10000]) / len(encode(text[:10000]))\n",
    "print(\"characters per token: \", chars_per_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_text = batch_encode(text, 200000)\n",
    "# # data = torch.tensor(encode(text), dtype=torch.int64)\n",
    "# data = torch.tensor(encoded_text, dtype=torch.int64, device='cuda')\n",
    "# print(data.dtype)\n",
    "# print(data.size())\n",
    "# print(data.device)\n",
    "# torch.save(data, 'tiny-stories-train.pt')\n",
    "# encoded_text = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from tiny-stories-train.pt\n",
    "data = torch.load('tiny-stories-train.pt', map_location='cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "468163695"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(data))\n",
    "\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([421347325])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 427,  357,   11,  258,  405,  452,  507,  365,  600,  258, 3729,  316,\n",
       "         308,  763,   13,  312,  708,  303,  281, 2965,  265,  360,  342,  303,\n",
       "         792,  303,  281, 2120,   13,  365,  450,  265,  953,  262, 3729,  342,\n",
       "         308,  367,   11,  350,  338,  466, 5179,  258, 2227,  345,  308, 2498,\n",
       "          13,  198,  343,  475,  265,  308,  367,  264,  326,   11,  328,  775,\n",
       "          11,  335,  600,  745, 3729,   13, 1283,  346,  953,  303,  342,  525,\n",
       "         264, 5179,  656, 2498,  484,  870,  367,  505,  264,  326,   11,  328,\n",
       "         835,   11,  365,   11,  368,  478,  953,  262, 3729,  264, 1307,  633,\n",
       "        2498,  421,  198, 4611,   11,  364, 1658,  262, 3729,  264, 7866,  262,\n",
       "        2227,  345,  365,  374, 2498,   13,  415,  281,  393, 2965,  369,  454,\n",
       "         792,  364,  435, 2500,  264, 1763,  761,  576,   13, 1454,  364, 1444,\n",
       "          11,  365,  863,  308,  367,  369, 2500,  262, 3729,  264, 5132,  308,\n",
       "        2498,   13,  320,  900,  520,  411,  792,  364,  361, 1658,  264, 1370,\n",
       "         570,   13,  198,  379,  382,  380,  198,  437,  453,  258,  404,   11,\n",
       "         407,  281,  258,  405,  568,  507, 2630,  596,   13, 2630,  596,  510,\n",
       "         265,  442,  852,  264,  360,  316,  262,  738,   13, 2630,  596,  281,\n",
       "         258, 2187,  568,  792,  278,  671,  361,  594, 4815,   13, 5185, 4815,\n",
       "         567, 2630,  596,  411,  264, 1124,   13,  198,  427,  357,   11, 2630,\n",
       "         596,  281, 3472,  316,  262,  573,  593,  278,  419,  258,  413,  684,\n",
       "          13,  298,  684,  361,  795, 1510,  358,  435, 4164,   13, 2630,  596,\n",
       "         620,  717,  262, 1510, 1546,  264,  450,  265,  360,  342,  454,   13,\n",
       "        2630,  596, 1848,  893,  262], device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:T+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\\nLily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\\nTogether, they shared the needle and sewed the button on Lily\\'s shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.\\n<|endoftext|>\\nOnce upon a time, there was a little car named Beep. Beep loved to go fast and play in the sun. Beep was a healthy car because he always had good fuel. Good fuel made Beep happy and strong.\\nOne day, Beep was driving in the park when he saw a big tree. The tree had many leaves that were falling. Beep liked how the leaves fall and wanted to play with them. Beep drove under the'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(train_data[:T+1].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_data[:T]\n",
    "y = train_data[1:T+1]\n",
    "for t in range(T):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    # print(\"when we see the text\", context, \"we predict the next character is\", target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(1337)\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(0, data.size(0) - T, (B,)) # 4 random locations we can sample from\n",
    "    x = torch.stack([data[i:i+T] for i in ix]) # random sequences\n",
    "    y = torch.stack([data[i+1:i+T+1] for i in ix]) # next character for each random sequence\n",
    "\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "\n",
    "for b in range(B):\n",
    "    for t in range(T): # for each of the characters in the sample\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "# torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    '''One Head of self-attention'''\n",
    "    def __init__(self, H):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(C, H, bias=False)\n",
    "        self.key = nn.Linear(C, H, bias=False)\n",
    "        self.value = nn.Linear(C, H, bias=False)\n",
    "        # self.output = nn.Linear(H, C, bias=False) # output matrix\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(T, T)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Query and Key matrices for the attention mechanism\n",
    "        # x: 8 tokens\n",
    "        # Q: 16 tall (arbitrary), 32 long channels\n",
    "        # K: 16 tall (arbitrary), 32 long channels\n",
    "\n",
    "        query_vectors = self.query(x)\n",
    "        key_vectors = self.key(x)\n",
    "\n",
    "\n",
    "        # Attention masking(so we can't look into the past):\n",
    "\n",
    "        tril = self.tril\n",
    "        wei = torch.zeros(T, T) \n",
    "        wei = wei.masked_fill(tril == 0, float('-inf')) # set the upper triangular to -inf\n",
    "        # xbow = wei @ x # apply the mask to the input, bag of words because simple avg.\n",
    "\n",
    "        # multiply the two to get the attention weights\n",
    "        attention_pattern = query_vectors @ key_vectors.transpose(-2, -1) # T, T\n",
    "        attention_pattern = attention_pattern / (H ** 0.5) # scale the attention pattern for numerical stability\n",
    "        attention_weights = F.softmax(attention_pattern + wei, dim=-1) # T, T (the row dimension is the query)\n",
    "\n",
    "        value_vectors = self.value(x) # the direction we should go in the embedding space for each token (ie more blue) T, H\n",
    "\n",
    "        # apply the attention weights to the value vectors\n",
    "        context = attention_weights @ value_vectors # T, H\n",
    "\n",
    "        # project back into original space from value space\n",
    "        # return self.output(context)\n",
    "        return context\n",
    "\n",
    "x = torch.randn(B,T,C)\n",
    "head = Head(H)\n",
    "# head(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    '''Multiple heads of self-attention'''\n",
    "    def __init__(self, H, C, n_heads): # H is head embedding space size, n_heads is number of heads\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(H) for _ in range(n_heads)])\n",
    "        self.combine_heads = nn.Linear(H*n_heads, C)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        x = self.combine_heads(x)  # T, C\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 256, 32])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head = MultiHeadAttention(H, C, n_heads)\n",
    "head.heads[0].forward(x).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    '''Feed-forward neural network'''\n",
    "    def __init__(self, C):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(C, C * feedforward_factor),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(C * feedforward_factor, C),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    '''Layer normalization'''\n",
    "    def __init__(self, C, use_affine=True):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(C)) if use_affine else None\n",
    "        self.beta = nn.Parameter(torch.zeros(C)) if use_affine else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        if self.gamma is not None and self.beta is not None:\n",
    "            return self.gamma * (x - mean) / (std + 1e-6) + self.beta\n",
    "        else:\n",
    "            return (x - mean) / (std + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    '''Transformer block'''\n",
    "    def __init__(self, H, C, n_heads):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(H, C, n_heads)\n",
    "        self.ff = FeedForward(C)\n",
    "        self.norm1 = LayerNorm(C, use_affine=True)\n",
    "        self.norm2 = LayerNorm(C, use_affine=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attention(self.norm1(x))\n",
    "        x = x + self.ff(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.7766, -2.0111, -0.3307,  ..., -0.3052,  0.5329, -1.4151],\n",
       "          [-1.7299, -1.4603, -1.6055,  ..., -0.2366, -0.2037, -1.5359],\n",
       "          [-1.0830, -2.4659, -1.2272,  ...,  0.0308,  0.3193, -1.6621],\n",
       "          ...,\n",
       "          [-2.7262, -0.9978, -1.5855,  ..., -0.6339,  1.3873, -2.6267],\n",
       "          [-1.9410, -1.2640, -1.2534,  ..., -0.3961, -0.7176, -0.7522],\n",
       "          [-1.3937, -1.2247, -0.4461,  ..., -0.0669,  1.0512, -0.7065]]],\n",
       "        device='cuda:0', grad_fn=<ViewBackward0>),\n",
       " None,\n",
       " tensor(0.1005, device='cuda:0', grad_fn=<DivBackward0>))"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, n_layers):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, C) \n",
    "        self.position_embedding_table = nn.Embedding(T, C)\n",
    "        self.lm_head = nn.Linear(C, vocab_size)\n",
    "        self.layers = nn.ModuleList([Block(H, C, n_heads) for _ in range(n_layers)])\n",
    "    \n",
    "    def forward(self, idx, targets=None, return_residuals=None):\n",
    "        B, T = idx.shape\n",
    "        token_emb = self.token_embedding_table(idx) # batch_dim, sequence_dim, embedding_dim\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T))\n",
    "        x = token_emb + pos_emb # token identities and positions contained\n",
    "\n",
    "        if return_residuals == \"first_embedding\":\n",
    "            return x\n",
    "\n",
    "        def excess_kurtosis(emb):\n",
    "            mean = torch.mean(emb, dim=-1, keepdim=True) # BxTx1\n",
    "            std = torch.std(emb, dim=-1, keepdim=True) # BxTx1\n",
    "\n",
    "            centralized = emb - mean #BxTxC\n",
    "            fourth_moment = torch.mean(centralized**4, dim=-1, keepdim=True) # BxTx1\n",
    "            kurtosis = torch.squeeze(fourth_moment / std**4, dim=-1) # BxT\n",
    "            # view as a 1d vector\n",
    "            kurtosis = kurtosis.view(-1) - 3\n",
    "            # make each one min 0\n",
    "            kurtosis = torch.maximum(kurtosis, torch.tensor(0.0))\n",
    "            # sum over the vector\n",
    "            kurtosis = torch.sum(kurtosis)\n",
    "            return kurtosis\n",
    "\n",
    "\n",
    "        kurtosis_sum = torch.tensor(0.0)\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "            kurtosis_sum += excess_kurtosis(x)\n",
    "            if return_residuals is not None and i == return_residuals:\n",
    "                return x\n",
    "        \n",
    "        kurtosis_avg = kurtosis_sum / (len(self.layers) * T * B)\n",
    "        # kurtosis_avg = torch.tensor(0.0)\n",
    "\n",
    "        logits = self.lm_head(x) # batch_dim, sequence_dim, vocab_size\n",
    "\n",
    "        batch_dim, sequence_dim, embedding_dim = logits.size()\n",
    "\n",
    "        # loss = F.cross_entropy(logits, targets) this won't work because we need 1d logits and 1d targets\n",
    "        # one-hot-vectors are a line in the x-dimension, so the shape of shape of the logits should be (-1, vocab_size).\n",
    "\n",
    "        if targets is None:\n",
    "            return logits, None, kurtosis_avg\n",
    "        else:\n",
    "            # a list of all the predictions, reguardles of batch.\n",
    "            # xdim: probabilities of each character in the vocab (embedding_dim=vocab_size)\n",
    "            # ydim: all predictions for all batches flattened (batch_dim*sequence_dim)\n",
    "            logits_loss_view = logits.view(-1, vocab_size) \n",
    "            # targets loss view\n",
    "            # xdim: all targets for all batches flattened (batch_dim*sequence_dim)\n",
    "            # so this would be like, [1,4,5,1,2,3, ...]\n",
    "            # where each number is the correct next index of the one hot vector\n",
    "            targets_loss_view = targets.view(-1)\n",
    "            loss = F.cross_entropy(logits_loss_view, targets_loss_view)\n",
    "            return logits, loss, kurtosis_avg\n",
    "\n",
    "    def generate(self, idx, max_new_tokens, temperature=0.5):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx[:,-T:])\n",
    "            # get the predictions of the last token\n",
    "            last_token_logits = logits[:, -1, :] # all batches, last token, all probabilities\n",
    "            # apply temperature\n",
    "            last_token_logits = last_token_logits / temperature\n",
    "            # softmax to get probabilities\n",
    "            probabilities = F.softmax(last_token_logits, dim=-1)\n",
    "            # sample from the probabilities\n",
    "            next_token = torch.multinomial(probabilities, num_samples=1)\n",
    "            # add the new token to the idx tensor\n",
    "            idx = torch.cat((idx, next_token), dim=1)\n",
    "        return idx\n",
    "    def prompt_model(self, prompt, max_new_tokens, temperature=0.5):\n",
    "        autoregressive_seq = encode(prompt)\n",
    "        for _ in range(max_new_tokens):\n",
    "            prediction_index = len(autoregressive_seq)-1\n",
    "\n",
    "            model_input = torch.tensor(autoregressive_seq)\n",
    "            \n",
    "            while model_input.shape[0] < T:\n",
    "                pad_token = torch.tensor(encode(\"\\n\"))\n",
    "                model_input = torch.cat((model_input, pad_token), dim=0)\n",
    "\n",
    "            model_input\n",
    "            model_input = model_input.unsqueeze(0)\n",
    "\n",
    "            logits, loss, kurtosis_avg = model(model_input)\n",
    "            prediction_token = logits[:, prediction_index, :] / temperature\n",
    "            probabilities = F.softmax(prediction_token, dim=-1)\n",
    "            next_token = torch.multinomial(probabilities, num_samples=1)\n",
    "            next_token = next_token.item()\n",
    "\n",
    "            autoregressive_seq.append(next_token)\n",
    "        # get the autoregressive sequence\n",
    "        return decode(autoregressive_seq)\n",
    "    def get_embedding(self, prompt, override_model_embedding_layer=None):\n",
    "        if override_model_embedding_layer is None:\n",
    "            selected_model_embedding_layer = model_embedding_layer\n",
    "        else:\n",
    "            selected_model_embedding_layer = override_model_embedding_layer\n",
    "        sequence = encode(prompt)\n",
    "        model_input = torch.tensor(sequence)\n",
    "        sequence_index = len(sequence) - 1\n",
    "        while model_input.shape[0] < T:\n",
    "            pad_token = torch.tensor(encode(\"\\n\"))\n",
    "            model_input = torch.cat((model_input, pad_token), dim=0)\n",
    "        model_input = model_input.unsqueeze(0)\n",
    "        embedding = self.forward(model_input, return_residuals=selected_model_embedding_layer)\n",
    "        # remove the batch dimension\n",
    "        embedding = embedding.squeeze(0)[sequence_index]\n",
    "        return embedding\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "model = GPT(n_layers)\n",
    "# logits, loss, kurtosis_avg = model(xb, yb)\n",
    "# print(logits.shape)\n",
    "# print(loss)\n",
    "# print(kurtosis_avg)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_idx = torch.zeros(1, T).long()\n",
    "model.forward(idx=test_idx)\n",
    "# decode(model.generate(idx=test_idx, max_new_tokens=100)[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (token_embedding_table): Embedding(8192, 256)\n",
       "  (position_embedding_table): Embedding(256, 256)\n",
       "  (lm_head): Linear(in_features=256, out_features=8192, bias=True)\n",
       "  (layers): ModuleList(\n",
       "    (0-11): 12 x Block(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (query): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (key): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (value): Linear(in_features=256, out_features=32, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (combine_heads): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=768, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=768, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters in the model:  12160000\n"
     ]
    }
   ],
   "source": [
    "# get the number of parameters in the model\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"number of parameters in the model: \", count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([468163695])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0]], device='cuda:0')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# logits, loss = self(idx[:,-T:])\n",
    "\n",
    "idx = torch.zeros(1, 1).long()\n",
    "idx[:,-T:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.token_embedding_table.weight.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_iters = 10\n",
    "eval_interval = 300\n",
    "@torch.no_grad()\n",
    "def estimate_loss(is_last=False):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        real_iters = eval_iters\n",
    "        if is_last and split == 'val':  # increase last eval to mitigate noise\n",
    "            real_iters *= 10 \n",
    "        losses = torch.zeros(real_iters)\n",
    "        for k in range(real_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss, kurtosis_avg = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean() / chars_per_token\n",
    "    model.train()\n",
    "    return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameter_to_data_ratio=0.02885980111538622\n",
      "token_embedding_table.weight: 2097152\n",
      "lm_head.weight: 2097152\n",
      "layers.0.ff.net.0.weight: 196608\n",
      "layers.0.ff.net.2.weight: 196608\n",
      "layers.1.ff.net.0.weight: 196608\n",
      "layers.1.ff.net.2.weight: 196608\n",
      "layers.2.ff.net.0.weight: 196608\n",
      "layers.2.ff.net.2.weight: 196608\n",
      "layers.3.ff.net.0.weight: 196608\n",
      "layers.3.ff.net.2.weight: 196608\n",
      "layers.4.ff.net.0.weight: 196608\n",
      "layers.4.ff.net.2.weight: 196608\n",
      "layers.5.ff.net.0.weight: 196608\n",
      "layers.5.ff.net.2.weight: 196608\n",
      "layers.6.ff.net.0.weight: 196608\n",
      "layers.6.ff.net.2.weight: 196608\n",
      "layers.7.ff.net.0.weight: 196608\n",
      "layers.7.ff.net.2.weight: 196608\n",
      "layers.8.ff.net.0.weight: 196608\n",
      "layers.8.ff.net.2.weight: 196608\n",
      "layers.9.ff.net.0.weight: 196608\n",
      "layers.9.ff.net.2.weight: 196608\n",
      "layers.10.ff.net.0.weight: 196608\n",
      "layers.10.ff.net.2.weight: 196608\n",
      "layers.11.ff.net.0.weight: 196608\n",
      "layers.11.ff.net.2.weight: 196608\n",
      "position_embedding_table.weight: 65536\n",
      "layers.0.attention.combine_heads.weight: 65536\n",
      "layers.1.attention.combine_heads.weight: 65536\n",
      "layers.2.attention.combine_heads.weight: 65536\n",
      "layers.3.attention.combine_heads.weight: 65536\n",
      "layers.4.attention.combine_heads.weight: 65536\n",
      "layers.5.attention.combine_heads.weight: 65536\n",
      "layers.6.attention.combine_heads.weight: 65536\n",
      "layers.7.attention.combine_heads.weight: 65536\n",
      "layers.8.attention.combine_heads.weight: 65536\n",
      "layers.9.attention.combine_heads.weight: 65536\n",
      "layers.10.attention.combine_heads.weight: 65536\n",
      "layers.11.attention.combine_heads.weight: 65536\n",
      "lm_head.bias: 8192\n",
      "layers.0.attention.heads.0.query.weight: 8192\n",
      "layers.0.attention.heads.0.key.weight: 8192\n",
      "layers.0.attention.heads.0.value.weight: 8192\n",
      "layers.0.attention.heads.1.query.weight: 8192\n",
      "layers.0.attention.heads.1.key.weight: 8192\n",
      "layers.0.attention.heads.1.value.weight: 8192\n",
      "layers.0.attention.heads.2.query.weight: 8192\n",
      "layers.0.attention.heads.2.key.weight: 8192\n",
      "layers.0.attention.heads.2.value.weight: 8192\n",
      "layers.0.attention.heads.3.query.weight: 8192\n",
      "layers.0.attention.heads.3.key.weight: 8192\n",
      "layers.0.attention.heads.3.value.weight: 8192\n",
      "layers.0.attention.heads.4.query.weight: 8192\n",
      "layers.0.attention.heads.4.key.weight: 8192\n",
      "layers.0.attention.heads.4.value.weight: 8192\n",
      "layers.0.attention.heads.5.query.weight: 8192\n",
      "layers.0.attention.heads.5.key.weight: 8192\n",
      "layers.0.attention.heads.5.value.weight: 8192\n",
      "layers.0.attention.heads.6.query.weight: 8192\n",
      "layers.0.attention.heads.6.key.weight: 8192\n",
      "layers.0.attention.heads.6.value.weight: 8192\n",
      "layers.0.attention.heads.7.query.weight: 8192\n",
      "layers.0.attention.heads.7.key.weight: 8192\n",
      "layers.0.attention.heads.7.value.weight: 8192\n",
      "layers.1.attention.heads.0.query.weight: 8192\n",
      "layers.1.attention.heads.0.key.weight: 8192\n",
      "layers.1.attention.heads.0.value.weight: 8192\n",
      "layers.1.attention.heads.1.query.weight: 8192\n",
      "layers.1.attention.heads.1.key.weight: 8192\n",
      "layers.1.attention.heads.1.value.weight: 8192\n",
      "layers.1.attention.heads.2.query.weight: 8192\n",
      "layers.1.attention.heads.2.key.weight: 8192\n",
      "layers.1.attention.heads.2.value.weight: 8192\n",
      "layers.1.attention.heads.3.query.weight: 8192\n",
      "layers.1.attention.heads.3.key.weight: 8192\n",
      "layers.1.attention.heads.3.value.weight: 8192\n",
      "layers.1.attention.heads.4.query.weight: 8192\n",
      "layers.1.attention.heads.4.key.weight: 8192\n",
      "layers.1.attention.heads.4.value.weight: 8192\n",
      "layers.1.attention.heads.5.query.weight: 8192\n",
      "layers.1.attention.heads.5.key.weight: 8192\n",
      "layers.1.attention.heads.5.value.weight: 8192\n",
      "layers.1.attention.heads.6.query.weight: 8192\n",
      "layers.1.attention.heads.6.key.weight: 8192\n",
      "layers.1.attention.heads.6.value.weight: 8192\n",
      "layers.1.attention.heads.7.query.weight: 8192\n",
      "layers.1.attention.heads.7.key.weight: 8192\n",
      "layers.1.attention.heads.7.value.weight: 8192\n",
      "layers.2.attention.heads.0.query.weight: 8192\n",
      "layers.2.attention.heads.0.key.weight: 8192\n",
      "layers.2.attention.heads.0.value.weight: 8192\n",
      "layers.2.attention.heads.1.query.weight: 8192\n",
      "layers.2.attention.heads.1.key.weight: 8192\n",
      "layers.2.attention.heads.1.value.weight: 8192\n",
      "layers.2.attention.heads.2.query.weight: 8192\n",
      "layers.2.attention.heads.2.key.weight: 8192\n",
      "layers.2.attention.heads.2.value.weight: 8192\n",
      "layers.2.attention.heads.3.query.weight: 8192\n",
      "layers.2.attention.heads.3.key.weight: 8192\n",
      "layers.2.attention.heads.3.value.weight: 8192\n",
      "layers.2.attention.heads.4.query.weight: 8192\n",
      "layers.2.attention.heads.4.key.weight: 8192\n",
      "layers.2.attention.heads.4.value.weight: 8192\n",
      "layers.2.attention.heads.5.query.weight: 8192\n",
      "layers.2.attention.heads.5.key.weight: 8192\n",
      "layers.2.attention.heads.5.value.weight: 8192\n",
      "layers.2.attention.heads.6.query.weight: 8192\n",
      "layers.2.attention.heads.6.key.weight: 8192\n",
      "layers.2.attention.heads.6.value.weight: 8192\n",
      "layers.2.attention.heads.7.query.weight: 8192\n",
      "layers.2.attention.heads.7.key.weight: 8192\n",
      "layers.2.attention.heads.7.value.weight: 8192\n",
      "layers.3.attention.heads.0.query.weight: 8192\n",
      "layers.3.attention.heads.0.key.weight: 8192\n",
      "layers.3.attention.heads.0.value.weight: 8192\n",
      "layers.3.attention.heads.1.query.weight: 8192\n",
      "layers.3.attention.heads.1.key.weight: 8192\n",
      "layers.3.attention.heads.1.value.weight: 8192\n",
      "layers.3.attention.heads.2.query.weight: 8192\n",
      "layers.3.attention.heads.2.key.weight: 8192\n",
      "layers.3.attention.heads.2.value.weight: 8192\n",
      "layers.3.attention.heads.3.query.weight: 8192\n",
      "layers.3.attention.heads.3.key.weight: 8192\n",
      "layers.3.attention.heads.3.value.weight: 8192\n",
      "layers.3.attention.heads.4.query.weight: 8192\n",
      "layers.3.attention.heads.4.key.weight: 8192\n",
      "layers.3.attention.heads.4.value.weight: 8192\n",
      "layers.3.attention.heads.5.query.weight: 8192\n",
      "layers.3.attention.heads.5.key.weight: 8192\n",
      "layers.3.attention.heads.5.value.weight: 8192\n",
      "layers.3.attention.heads.6.query.weight: 8192\n",
      "layers.3.attention.heads.6.key.weight: 8192\n",
      "layers.3.attention.heads.6.value.weight: 8192\n",
      "layers.3.attention.heads.7.query.weight: 8192\n",
      "layers.3.attention.heads.7.key.weight: 8192\n",
      "layers.3.attention.heads.7.value.weight: 8192\n",
      "layers.4.attention.heads.0.query.weight: 8192\n",
      "layers.4.attention.heads.0.key.weight: 8192\n",
      "layers.4.attention.heads.0.value.weight: 8192\n",
      "layers.4.attention.heads.1.query.weight: 8192\n",
      "layers.4.attention.heads.1.key.weight: 8192\n",
      "layers.4.attention.heads.1.value.weight: 8192\n",
      "layers.4.attention.heads.2.query.weight: 8192\n",
      "layers.4.attention.heads.2.key.weight: 8192\n",
      "layers.4.attention.heads.2.value.weight: 8192\n",
      "layers.4.attention.heads.3.query.weight: 8192\n",
      "layers.4.attention.heads.3.key.weight: 8192\n",
      "layers.4.attention.heads.3.value.weight: 8192\n",
      "layers.4.attention.heads.4.query.weight: 8192\n",
      "layers.4.attention.heads.4.key.weight: 8192\n",
      "layers.4.attention.heads.4.value.weight: 8192\n",
      "layers.4.attention.heads.5.query.weight: 8192\n",
      "layers.4.attention.heads.5.key.weight: 8192\n",
      "layers.4.attention.heads.5.value.weight: 8192\n",
      "layers.4.attention.heads.6.query.weight: 8192\n",
      "layers.4.attention.heads.6.key.weight: 8192\n",
      "layers.4.attention.heads.6.value.weight: 8192\n",
      "layers.4.attention.heads.7.query.weight: 8192\n",
      "layers.4.attention.heads.7.key.weight: 8192\n",
      "layers.4.attention.heads.7.value.weight: 8192\n",
      "layers.5.attention.heads.0.query.weight: 8192\n",
      "layers.5.attention.heads.0.key.weight: 8192\n",
      "layers.5.attention.heads.0.value.weight: 8192\n",
      "layers.5.attention.heads.1.query.weight: 8192\n",
      "layers.5.attention.heads.1.key.weight: 8192\n",
      "layers.5.attention.heads.1.value.weight: 8192\n",
      "layers.5.attention.heads.2.query.weight: 8192\n",
      "layers.5.attention.heads.2.key.weight: 8192\n",
      "layers.5.attention.heads.2.value.weight: 8192\n",
      "layers.5.attention.heads.3.query.weight: 8192\n",
      "layers.5.attention.heads.3.key.weight: 8192\n",
      "layers.5.attention.heads.3.value.weight: 8192\n",
      "layers.5.attention.heads.4.query.weight: 8192\n",
      "layers.5.attention.heads.4.key.weight: 8192\n",
      "layers.5.attention.heads.4.value.weight: 8192\n",
      "layers.5.attention.heads.5.query.weight: 8192\n",
      "layers.5.attention.heads.5.key.weight: 8192\n",
      "layers.5.attention.heads.5.value.weight: 8192\n",
      "layers.5.attention.heads.6.query.weight: 8192\n",
      "layers.5.attention.heads.6.key.weight: 8192\n",
      "layers.5.attention.heads.6.value.weight: 8192\n",
      "layers.5.attention.heads.7.query.weight: 8192\n",
      "layers.5.attention.heads.7.key.weight: 8192\n",
      "layers.5.attention.heads.7.value.weight: 8192\n",
      "layers.6.attention.heads.0.query.weight: 8192\n",
      "layers.6.attention.heads.0.key.weight: 8192\n",
      "layers.6.attention.heads.0.value.weight: 8192\n",
      "layers.6.attention.heads.1.query.weight: 8192\n",
      "layers.6.attention.heads.1.key.weight: 8192\n",
      "layers.6.attention.heads.1.value.weight: 8192\n",
      "layers.6.attention.heads.2.query.weight: 8192\n",
      "layers.6.attention.heads.2.key.weight: 8192\n",
      "layers.6.attention.heads.2.value.weight: 8192\n",
      "layers.6.attention.heads.3.query.weight: 8192\n",
      "layers.6.attention.heads.3.key.weight: 8192\n",
      "layers.6.attention.heads.3.value.weight: 8192\n",
      "layers.6.attention.heads.4.query.weight: 8192\n",
      "layers.6.attention.heads.4.key.weight: 8192\n",
      "layers.6.attention.heads.4.value.weight: 8192\n",
      "layers.6.attention.heads.5.query.weight: 8192\n",
      "layers.6.attention.heads.5.key.weight: 8192\n",
      "layers.6.attention.heads.5.value.weight: 8192\n",
      "layers.6.attention.heads.6.query.weight: 8192\n",
      "layers.6.attention.heads.6.key.weight: 8192\n",
      "layers.6.attention.heads.6.value.weight: 8192\n",
      "layers.6.attention.heads.7.query.weight: 8192\n",
      "layers.6.attention.heads.7.key.weight: 8192\n",
      "layers.6.attention.heads.7.value.weight: 8192\n",
      "layers.7.attention.heads.0.query.weight: 8192\n",
      "layers.7.attention.heads.0.key.weight: 8192\n",
      "layers.7.attention.heads.0.value.weight: 8192\n",
      "layers.7.attention.heads.1.query.weight: 8192\n",
      "layers.7.attention.heads.1.key.weight: 8192\n",
      "layers.7.attention.heads.1.value.weight: 8192\n",
      "layers.7.attention.heads.2.query.weight: 8192\n",
      "layers.7.attention.heads.2.key.weight: 8192\n",
      "layers.7.attention.heads.2.value.weight: 8192\n",
      "layers.7.attention.heads.3.query.weight: 8192\n",
      "layers.7.attention.heads.3.key.weight: 8192\n",
      "layers.7.attention.heads.3.value.weight: 8192\n",
      "layers.7.attention.heads.4.query.weight: 8192\n",
      "layers.7.attention.heads.4.key.weight: 8192\n",
      "layers.7.attention.heads.4.value.weight: 8192\n",
      "layers.7.attention.heads.5.query.weight: 8192\n",
      "layers.7.attention.heads.5.key.weight: 8192\n",
      "layers.7.attention.heads.5.value.weight: 8192\n",
      "layers.7.attention.heads.6.query.weight: 8192\n",
      "layers.7.attention.heads.6.key.weight: 8192\n",
      "layers.7.attention.heads.6.value.weight: 8192\n",
      "layers.7.attention.heads.7.query.weight: 8192\n",
      "layers.7.attention.heads.7.key.weight: 8192\n",
      "layers.7.attention.heads.7.value.weight: 8192\n",
      "layers.8.attention.heads.0.query.weight: 8192\n",
      "layers.8.attention.heads.0.key.weight: 8192\n",
      "layers.8.attention.heads.0.value.weight: 8192\n",
      "layers.8.attention.heads.1.query.weight: 8192\n",
      "layers.8.attention.heads.1.key.weight: 8192\n",
      "layers.8.attention.heads.1.value.weight: 8192\n",
      "layers.8.attention.heads.2.query.weight: 8192\n",
      "layers.8.attention.heads.2.key.weight: 8192\n",
      "layers.8.attention.heads.2.value.weight: 8192\n",
      "layers.8.attention.heads.3.query.weight: 8192\n",
      "layers.8.attention.heads.3.key.weight: 8192\n",
      "layers.8.attention.heads.3.value.weight: 8192\n",
      "layers.8.attention.heads.4.query.weight: 8192\n",
      "layers.8.attention.heads.4.key.weight: 8192\n",
      "layers.8.attention.heads.4.value.weight: 8192\n",
      "layers.8.attention.heads.5.query.weight: 8192\n",
      "layers.8.attention.heads.5.key.weight: 8192\n",
      "layers.8.attention.heads.5.value.weight: 8192\n",
      "layers.8.attention.heads.6.query.weight: 8192\n",
      "layers.8.attention.heads.6.key.weight: 8192\n",
      "layers.8.attention.heads.6.value.weight: 8192\n",
      "layers.8.attention.heads.7.query.weight: 8192\n",
      "layers.8.attention.heads.7.key.weight: 8192\n",
      "layers.8.attention.heads.7.value.weight: 8192\n",
      "layers.9.attention.heads.0.query.weight: 8192\n",
      "layers.9.attention.heads.0.key.weight: 8192\n",
      "layers.9.attention.heads.0.value.weight: 8192\n",
      "layers.9.attention.heads.1.query.weight: 8192\n",
      "layers.9.attention.heads.1.key.weight: 8192\n",
      "layers.9.attention.heads.1.value.weight: 8192\n",
      "layers.9.attention.heads.2.query.weight: 8192\n",
      "layers.9.attention.heads.2.key.weight: 8192\n",
      "layers.9.attention.heads.2.value.weight: 8192\n",
      "layers.9.attention.heads.3.query.weight: 8192\n",
      "layers.9.attention.heads.3.key.weight: 8192\n",
      "layers.9.attention.heads.3.value.weight: 8192\n",
      "layers.9.attention.heads.4.query.weight: 8192\n",
      "layers.9.attention.heads.4.key.weight: 8192\n",
      "layers.9.attention.heads.4.value.weight: 8192\n",
      "layers.9.attention.heads.5.query.weight: 8192\n",
      "layers.9.attention.heads.5.key.weight: 8192\n",
      "layers.9.attention.heads.5.value.weight: 8192\n",
      "layers.9.attention.heads.6.query.weight: 8192\n",
      "layers.9.attention.heads.6.key.weight: 8192\n",
      "layers.9.attention.heads.6.value.weight: 8192\n",
      "layers.9.attention.heads.7.query.weight: 8192\n",
      "layers.9.attention.heads.7.key.weight: 8192\n",
      "layers.9.attention.heads.7.value.weight: 8192\n",
      "layers.10.attention.heads.0.query.weight: 8192\n",
      "layers.10.attention.heads.0.key.weight: 8192\n",
      "layers.10.attention.heads.0.value.weight: 8192\n",
      "layers.10.attention.heads.1.query.weight: 8192\n",
      "layers.10.attention.heads.1.key.weight: 8192\n",
      "layers.10.attention.heads.1.value.weight: 8192\n",
      "layers.10.attention.heads.2.query.weight: 8192\n",
      "layers.10.attention.heads.2.key.weight: 8192\n",
      "layers.10.attention.heads.2.value.weight: 8192\n",
      "layers.10.attention.heads.3.query.weight: 8192\n",
      "layers.10.attention.heads.3.key.weight: 8192\n",
      "layers.10.attention.heads.3.value.weight: 8192\n",
      "layers.10.attention.heads.4.query.weight: 8192\n",
      "layers.10.attention.heads.4.key.weight: 8192\n",
      "layers.10.attention.heads.4.value.weight: 8192\n",
      "layers.10.attention.heads.5.query.weight: 8192\n",
      "layers.10.attention.heads.5.key.weight: 8192\n",
      "layers.10.attention.heads.5.value.weight: 8192\n",
      "layers.10.attention.heads.6.query.weight: 8192\n",
      "layers.10.attention.heads.6.key.weight: 8192\n",
      "layers.10.attention.heads.6.value.weight: 8192\n",
      "layers.10.attention.heads.7.query.weight: 8192\n",
      "layers.10.attention.heads.7.key.weight: 8192\n",
      "layers.10.attention.heads.7.value.weight: 8192\n",
      "layers.11.attention.heads.0.query.weight: 8192\n",
      "layers.11.attention.heads.0.key.weight: 8192\n",
      "layers.11.attention.heads.0.value.weight: 8192\n",
      "layers.11.attention.heads.1.query.weight: 8192\n",
      "layers.11.attention.heads.1.key.weight: 8192\n",
      "layers.11.attention.heads.1.value.weight: 8192\n",
      "layers.11.attention.heads.2.query.weight: 8192\n",
      "layers.11.attention.heads.2.key.weight: 8192\n",
      "layers.11.attention.heads.2.value.weight: 8192\n",
      "layers.11.attention.heads.3.query.weight: 8192\n",
      "layers.11.attention.heads.3.key.weight: 8192\n",
      "layers.11.attention.heads.3.value.weight: 8192\n",
      "layers.11.attention.heads.4.query.weight: 8192\n",
      "layers.11.attention.heads.4.key.weight: 8192\n",
      "layers.11.attention.heads.4.value.weight: 8192\n",
      "layers.11.attention.heads.5.query.weight: 8192\n",
      "layers.11.attention.heads.5.key.weight: 8192\n",
      "layers.11.attention.heads.5.value.weight: 8192\n",
      "layers.11.attention.heads.6.query.weight: 8192\n",
      "layers.11.attention.heads.6.key.weight: 8192\n",
      "layers.11.attention.heads.6.value.weight: 8192\n",
      "layers.11.attention.heads.7.query.weight: 8192\n",
      "layers.11.attention.heads.7.key.weight: 8192\n",
      "layers.11.attention.heads.7.value.weight: 8192\n",
      "layers.0.ff.net.0.bias: 768\n",
      "layers.1.ff.net.0.bias: 768\n",
      "layers.2.ff.net.0.bias: 768\n",
      "layers.3.ff.net.0.bias: 768\n",
      "layers.4.ff.net.0.bias: 768\n",
      "layers.5.ff.net.0.bias: 768\n",
      "layers.6.ff.net.0.bias: 768\n",
      "layers.7.ff.net.0.bias: 768\n",
      "layers.8.ff.net.0.bias: 768\n",
      "layers.9.ff.net.0.bias: 768\n",
      "layers.10.ff.net.0.bias: 768\n",
      "layers.11.ff.net.0.bias: 768\n",
      "layers.0.attention.combine_heads.bias: 256\n",
      "layers.0.ff.net.2.bias: 256\n",
      "layers.0.norm1.gamma: 256\n",
      "layers.0.norm1.beta: 256\n",
      "layers.0.norm2.gamma: 256\n",
      "layers.0.norm2.beta: 256\n",
      "layers.1.attention.combine_heads.bias: 256\n",
      "layers.1.ff.net.2.bias: 256\n",
      "layers.1.norm1.gamma: 256\n",
      "layers.1.norm1.beta: 256\n",
      "layers.1.norm2.gamma: 256\n",
      "layers.1.norm2.beta: 256\n",
      "layers.2.attention.combine_heads.bias: 256\n",
      "layers.2.ff.net.2.bias: 256\n",
      "layers.2.norm1.gamma: 256\n",
      "layers.2.norm1.beta: 256\n",
      "layers.2.norm2.gamma: 256\n",
      "layers.2.norm2.beta: 256\n",
      "layers.3.attention.combine_heads.bias: 256\n",
      "layers.3.ff.net.2.bias: 256\n",
      "layers.3.norm1.gamma: 256\n",
      "layers.3.norm1.beta: 256\n",
      "layers.3.norm2.gamma: 256\n",
      "layers.3.norm2.beta: 256\n",
      "layers.4.attention.combine_heads.bias: 256\n",
      "layers.4.ff.net.2.bias: 256\n",
      "layers.4.norm1.gamma: 256\n",
      "layers.4.norm1.beta: 256\n",
      "layers.4.norm2.gamma: 256\n",
      "layers.4.norm2.beta: 256\n",
      "layers.5.attention.combine_heads.bias: 256\n",
      "layers.5.ff.net.2.bias: 256\n",
      "layers.5.norm1.gamma: 256\n",
      "layers.5.norm1.beta: 256\n",
      "layers.5.norm2.gamma: 256\n",
      "layers.5.norm2.beta: 256\n",
      "layers.6.attention.combine_heads.bias: 256\n",
      "layers.6.ff.net.2.bias: 256\n",
      "layers.6.norm1.gamma: 256\n",
      "layers.6.norm1.beta: 256\n",
      "layers.6.norm2.gamma: 256\n",
      "layers.6.norm2.beta: 256\n",
      "layers.7.attention.combine_heads.bias: 256\n",
      "layers.7.ff.net.2.bias: 256\n",
      "layers.7.norm1.gamma: 256\n",
      "layers.7.norm1.beta: 256\n",
      "layers.7.norm2.gamma: 256\n",
      "layers.7.norm2.beta: 256\n",
      "layers.8.attention.combine_heads.bias: 256\n",
      "layers.8.ff.net.2.bias: 256\n",
      "layers.8.norm1.gamma: 256\n",
      "layers.8.norm1.beta: 256\n",
      "layers.8.norm2.gamma: 256\n",
      "layers.8.norm2.beta: 256\n",
      "layers.9.attention.combine_heads.bias: 256\n",
      "layers.9.ff.net.2.bias: 256\n",
      "layers.9.norm1.gamma: 256\n",
      "layers.9.norm1.beta: 256\n",
      "layers.9.norm2.gamma: 256\n",
      "layers.9.norm2.beta: 256\n",
      "layers.10.attention.combine_heads.bias: 256\n",
      "layers.10.ff.net.2.bias: 256\n",
      "layers.10.norm1.gamma: 256\n",
      "layers.10.norm1.beta: 256\n",
      "layers.10.norm2.gamma: 256\n",
      "layers.10.norm2.beta: 256\n",
      "layers.11.attention.combine_heads.bias: 256\n",
      "layers.11.ff.net.2.bias: 256\n",
      "layers.11.norm1.gamma: 256\n",
      "layers.11.norm1.beta: 256\n",
      "layers.11.norm2.gamma: 256\n",
      "layers.11.norm2.beta: 256\n"
     ]
    }
   ],
   "source": [
    "# get the number of parameters\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "parameter_to_data_ratio = n_params / len(train_data)\n",
    "print(f\"{parameter_to_data_ratio=}\")\n",
    "\n",
    "parameters = []\n",
    "for name, param in model.named_parameters():\n",
    "    parameters.append({\"name\": name, \"params\": param.numel()})\n",
    "\n",
    "# sort parameters by size\n",
    "sorted_parameters = sorted(parameters, key=lambda x: x[\"params\"], reverse=True)\n",
    "for p in sorted_parameters:\n",
    "    print(f\"{p['name']}: {p['params']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_logs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/60000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/60000 [00:01<26:03:32,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 5.833105564117432, 'val': 5.829655170440674, 'kurtosis_avg': 0.1042330414056778}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 302/60000 [00:59<7:17:41,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.7641372084617615, 'val': 0.7641605734825134, 'kurtosis_avg': 0.08649500459432602}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 602/60000 [01:58<7:23:32,  2.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.6359838247299194, 'val': 0.6396847367286682, 'kurtosis_avg': 0.08682642877101898}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 901/60000 [02:59<9:36:11,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.5760225653648376, 'val': 0.5778737664222717, 'kurtosis_avg': 0.1038162112236023}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1201/60000 [04:01<9:24:12,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.5474318265914917, 'val': 0.5481188893318176, 'kurtosis_avg': 0.12645593285560608}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1502/60000 [05:00<6:54:36,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.5271503925323486, 'val': 0.52373206615448, 'kurtosis_avg': 0.16168904304504395}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1801/60000 [06:00<9:28:40,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.5143406391143799, 'val': 0.507614016532898, 'kurtosis_avg': 0.20325183868408203}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 2101/60000 [06:57<9:16:41,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.5010859966278076, 'val': 0.5000424981117249, 'kurtosis_avg': 0.2507350742816925}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2401/60000 [07:57<9:30:52,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.48555079102516174, 'val': 0.4834964871406555, 'kurtosis_avg': 0.30870962142944336}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 2701/60000 [08:58<9:31:07,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.4833345413208008, 'val': 0.48249122500419617, 'kurtosis_avg': 0.35835331678390503}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 3001/60000 [09:59<9:01:36,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.4727259576320648, 'val': 0.4755810797214508, 'kurtosis_avg': 0.4472466707229614}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3301/60000 [10:59<8:59:31,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.46942514181137085, 'val': 0.46946609020233154, 'kurtosis_avg': 0.5063214302062988}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3602/60000 [11:59<7:08:31,  2.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.4666297137737274, 'val': 0.4670414924621582, 'kurtosis_avg': 0.539631724357605}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 3901/60000 [13:01<8:58:28,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.4566399157047272, 'val': 0.46052688360214233, 'kurtosis_avg': 0.6101179122924805}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 4201/60000 [14:01<8:42:01,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.45902514457702637, 'val': 0.45584118366241455, 'kurtosis_avg': 0.6696665287017822}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4501/60000 [15:02<8:56:57,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.45348095893859863, 'val': 0.4496557116508484, 'kurtosis_avg': 0.729421079158783}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4802/60000 [16:00<6:08:29,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.4464639127254486, 'val': 0.44994765520095825, 'kurtosis_avg': 0.7882544994354248}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 5102/60000 [16:56<6:01:37,  2.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tIain': 0.4480448365211487, 'val': 0.4459271728992462, 'kurtosis_avg': 0.8221820592880249}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 5324/60000 [17:38<3:01:07,  5.03it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_15107/2848316066.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mtrain_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkurtosis_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mkurtosis_avg\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.4\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m160\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0meval_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    476\u001b[0m         \"\"\"\n\u001b[1;32m    477\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_unary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m             return handle_torch_function(\n\u001b[0m\u001b[1;32m    479\u001b[0m                 \u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/overrides.py\u001b[0m in \u001b[0;36mhandle_torch_function\u001b[0;34m(public_api, relevant_args, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1532\u001b[0m         \u001b[0;31m# this unsets it and calls directly into TorchFunctionStackMode's torch function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1533\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_pop_mode_temporarily\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1534\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__torch_function__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpublic_api\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1535\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1536\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/_device.py\u001b[0m in \u001b[0;36m__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_device_constructors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;31m# NB: This is directly called from C++ in torch/csrc/Device.cpp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.000001, momentum=0.9)\n",
    "# \"learning_rate\": 2e-3,\n",
    "\n",
    "import tqdm\n",
    "num_params = sum([p.numel() for p in model.parameters()])\n",
    "\n",
    "for steps in tqdm.tqdm(range(max_iters)):\n",
    "    xb, yb = get_batch('train')\n",
    "    # loss\n",
    "    logits, loss, kurtosis_avg = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    # l2 regularization\n",
    "    # l2 = sum(p.pow(2).sum() for p in model.parameters()) / num_params\n",
    "    train_logs.append((loss.item(), kurtosis_avg.item()))\n",
    "    loss = loss + kurtosis_avg * 0.4/160\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if steps % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        # wandb.log({\"tIain\": losses['train'].item(), \"val\": losses['val'].item(), \"l2\":l2})\n",
    "        print({\"tIain\": losses['train'].item(), \"val\": losses['val'].item(), \"kurtosis_avg\": kurtosis_avg.item()})\n",
    "\n",
    "losses = estimate_loss(is_last=True)\n",
    "# wandb.log({\"train\": losses['train'].item(), \"val\": losses['val'].item()})\n",
    "# wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x432 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss: 1.8262\n",
      "Final kurtosis: 0.8614\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract loss and kurtosis values from train_logs\n",
    "losses, kurtosis_values = zip(*train_logs)\n",
    "\n",
    "# Create a figure with one plot and two y-axes\n",
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Plot loss on the primary y-axis\n",
    "color = 'tab:blue'\n",
    "ax1.set_xlabel('Steps')\n",
    "ax1.set_ylabel('Loss', color=color)\n",
    "ax1.plot(losses, color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "# ax1.set_ylim(0, 16)\n",
    "\n",
    "# Draw a horizontal line at loss=0.52\n",
    "# ax1.axhline(y=0.52, color='r', linestyle='--', label='Loss = 0.52')\n",
    "\n",
    "# Create a secondary y-axis for kurtosis\n",
    "ax2 = ax1.twinx()\n",
    "color = 'tab:orange'\n",
    "ax2.set_ylabel('Kurtosis', color=color)\n",
    "ax2.plot(kurtosis_values, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "# ax2.set_ylim(0, 70)\n",
    "\n",
    "# Set the title and add legend\n",
    "plt.title('Training Loss and Average Kurtosis (Adam optimizer)')\n",
    "ax1.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print the final loss and kurtosis values\n",
    "print(f\"Final loss: {losses[-1]:.4f}\")\n",
    "print(f\"Final kurtosis: {kurtosis_values[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': tensor(0.4496, device='cuda:0'),\n",
       " 'val': tensor(0.4473, device='cuda:0')}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "# torch.save(model.state_dict(), 'tiny-stories-model-kurtosis-regularize-0.44-loss.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the model\n",
    "# model.load_state_dict(torch.load('models/tiny-stories-model-SGD-0.52-loss.pt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|>\n",
      "Once upon a time, there was a little boy named Timmy. Timmy loved to play with his toy cars and trucks. One day, Timmy's mom gave him a new toy car. It was a big red car with a red car. Timmy was very happy and played with his new toy cars all day long.\n",
      "But then, Timmy's mom said, \"Timmy, it's time to go home now.\" Timmy didn't want to leave his new car, but he knew his mom would be there soon. So, he went to his room and got his new car. He played with it all day long and had lots of fun. From that day on, Timmy always made sure to take his new car outside and keep it safe.\n",
      "<|endoftext|>\n",
      "Once upon a time, there was a little girl named Lily. She loved to play outside in the park with her friends. One day, they saw a big, ugly dog. The dog was very friendly and Lily wanted to pet it.\n",
      "Lily said, \"Please, please\n"
     ]
    }
   ],
   "source": [
    "print(model.prompt_model(\"<|endoftext|>\\nOnce upon a time, there was a little boy\", 200, 0.4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kurtosis debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of the highest value in emb1: 239\n",
      "Index of the highest value in emb2: 146\n",
      "Index of the lowest value in emb1: 203\n",
      "Index of the lowest value in emb2: 81\n",
      "emb1 excess kurtosis: -0.31801652908325195\n",
      "emb2 excess kurtosis: -0.1101229190826416\n",
      "Dot product between emb1 and emb2: 0.8099309206008911\n"
     ]
    }
   ],
   "source": [
    "story1='''Once upon a time, in a big forest, there lived a rhinoceros named Roxy. Roxy loved to climb. She climbed trees, rocks, and hills. One day, Roxy found an icy hill. She had never seen anything like it before. It was shiny and cold, and she wanted to climb it.\n",
    "Roxy tried to climb the icy hill, but it was very slippery. She tried again and again, but she kept falling down. Roxy was sad. She wanted to climb the icy hill so much. Then, she saw a little bird named Billy. Billy saw that Roxy was sad and asked, \"Why are you sad, Roxy?\"\n",
    "Roxy told Billy about the icy hill and how she couldn't climb it'''\n",
    "\n",
    "# assume BxTxC\n",
    "def excess_kurtosis(emb):\n",
    "    mean = torch.mean(emb, dim=-1, keepdim=True) # BxTx1\n",
    "    std = torch.std(emb, dim=-1, keepdim=True) # BxTx1\n",
    "\n",
    "    centralized = emb - mean #BxTxC\n",
    "    fourth_moment = torch.mean(centralized**4, dim=-1, keepdim=True) # BxTx1\n",
    "    kurtosis = torch.squeeze(fourth_moment / std**4, dim=-1) # BxT\n",
    "    return kurtosis - 3\n",
    "\n",
    "\n",
    "\n",
    "emb1 = model.get_embedding(\"Tim and Lily saw a big dog\", override_model_embedding_layer=6)\n",
    "emb2 = model.get_embedding(\"Lilly and Tim noticed a cat\", override_model_embedding_layer=6)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Plot emb1 and emb2 in the same plot\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(np.square(emb1.cpu().detach().numpy()), label='emb1', color='blue')\n",
    "# plt.plot(np.square(emb2.cpu().detach().numpy()), label='emb2', color='red')\n",
    "# plt.xlabel('Index')\n",
    "# plt.ylabel('Value')\n",
    "# plt.title('emb1 and emb2 Plot')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# get the index of the highest value\n",
    "# Assuming emb1 and emb2 are tensors\n",
    "highest_value_index_emb1 = torch.argmax(emb1).item()\n",
    "highest_value_index_emb2 = torch.argmax(emb2).item()\n",
    "\n",
    "lowest_value_index_emb1 = torch.argmin(emb1).item()\n",
    "lowest_value_index_emb2 = torch.argmin(emb2).item()\n",
    "\n",
    "print(f\"Index of the highest value in emb1: {highest_value_index_emb1}\")\n",
    "print(f\"Index of the highest value in emb2: {highest_value_index_emb2}\")\n",
    "print(f\"Index of the lowest value in emb1: {lowest_value_index_emb1}\")\n",
    "print(f\"Index of the lowest value in emb2: {lowest_value_index_emb2}\")\n",
    "\n",
    "print(f\"emb1 excess kurtosis: {excess_kurtosis(emb1)}\")\n",
    "print(f\"emb2 excess kurtosis: {excess_kurtosis(emb2)}\")\n",
    "\n",
    "# dot product between emb1 and emb2\n",
    "emb1_l2 = F.normalize(emb1, p=2, dim=-1)\n",
    "emb2_l2 = F.normalize(emb2, p=2, dim=-1)\n",
    "print(f\"Dot product between emb1 and emb2: {torch.dot(emb1_l2, emb2_l2)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "emb1 excess kurtosis: 157.1050262451172\n",
    "emb2 excess kurtosis: 156.85986328125\n",
    "when we load the model trained from this notebook, it has excess kurtosis of 157.1050262451172"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.3952, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "xb, yb = get_batch('val')\n",
    "emb = model.forward(xb, return_residuals=model_embedding_layer)\n",
    "\n",
    "# emb.shape\n",
    "kurtosis_values = excess_kurtosis(emb).view(-1)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert kurtosis values to CPU and numpy for plotting\n",
    "kurtosis_np = kurtosis_values.cpu().detach().numpy()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(kurtosis_np, bins=50, edgecolor='black')\n",
    "plt.xlabel('Excess Kurtosis')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Excess Kurtosis Values kurtosis regularized model at loss 0.44 ')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(kurtosis_values.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
